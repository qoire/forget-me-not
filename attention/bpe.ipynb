{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding\n",
    "- Start with a small vocabulary (for our ease of use let's just assume only ASCII characters)\n",
    "- \"Learn\" new vocabulary by scanning through corpus, use frequency of sub-sequence appearence to learn the most important new vocubulary\n",
    "\n",
    "For simplicity, let's limit our vocabulary to only uppercase and lowercase ASCII, plus numbers. We will first purge the corpus of all other symbols while preserving spaces, note that this is not a realistic scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 575352\n",
      "initial vocab size: 62\n",
      "initial vocab: {'x', 't', 'p', 'G', 'W', 'B', 'R', 'J', 'T', '0', 'L', 'e', '6', 'A', 'E', '3', '5', 'N', 'r', '9', 'S', 'w', 'q', 'v', '4', 'C', 'Y', '2', 'X', 'K', 'Z', 'V', 'd', 'j', 'l', 'i', 'c', 'o', 's', 'n', 'H', 'z', 'b', 'I', 'k', '7', 'D', 'F', 'g', 'P', '1', 'u', 'f', '8', 'O', 'U', 'y', 'a', 'M', 'h', 'm', 'Q'}\n"
     ]
    }
   ],
   "source": [
    "# Load corpus, read all lines into memory, clean, then extract all characters, whitespace symbols\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "WIKITEXT_PATH = \"../data/Salesforce/wikitext/wikitext-103-raw-v1\"\n",
    "TRAIN_SET = [\"train-00000-of-00002.parquet\", \"train-00001-of-00002.parquet\"]\n",
    "TRAIN_SET_PATHS = [os.path.join(WIKITEXT_PATH, p) for p in TRAIN_SET]\n",
    "corpus = pq.ParquetDataset(TRAIN_SET_PATHS).read().to_pandas()\n",
    "\n",
    "# Clean the corpus of whitespace symbols (assume they're not useful to us)\n",
    "clear_ws_pattern = re.compile(r\"[^a-zA-Z0-9]\")\n",
    "\n",
    "def clean_corpus(txt: str) -> str:\n",
    "    spaced_txt = clear_ws_pattern.sub(' ', txt)\n",
    "    return ' '.join(spaced_txt.strip().split())\n",
    "\n",
    "corpus[\"text_clean\"] = corpus[\"text\"].apply(clean_corpus)\n",
    "\n",
    "# Now start a dictionary of character counts (importance)\n",
    "char_count_dict = {\n",
    "    \"a\": 0  # as an example\n",
    "}\n",
    "\n",
    "def count_chars(txt: str) -> None:\n",
    "    global char_count_dict\n",
    "    # kind of like bag of words (bag of chars?)\n",
    "    for c in list(txt):\n",
    "        if c not in char_count_dict:\n",
    "            char_count_dict[c] = 0\n",
    "        char_count_dict[c] += 1\n",
    "\n",
    "# Okay now the \"cleaning\" is done, we can start applying character and word counts\n",
    "# Count character sum in corpus (not _really_ needded, just for viz)\n",
    "for index, txt in corpus[\"text_clean\"].items():\n",
    "    count_chars(txt)\n",
    "\n",
    "# Removal all empty rows\n",
    "corpus = corpus[corpus[\"text_clean\"].str.len() > 0]\n",
    "\n",
    "# Count characters, then sort into descending order, delete the space parameter (considered whitespace)\n",
    "# Note: the sorting is purely for visualization, it is not needed\n",
    "del char_count_dict[' ']\n",
    "char_count_dict = dict(sorted(char_count_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# This forms our initial character set\n",
    "# Note: why are we going through all this trouble to get a corpus? We originally extended to chars\n",
    "# beyond just alphanum\n",
    "char_set = set(char_count_dict.keys())\n",
    "\n",
    "# Now we need to generate our initial word set, same as char_count_dict\n",
    "# (basically bag-of-words)\n",
    "word_count_dict = {}\n",
    "\n",
    "def count_words(txt: str) -> None:\n",
    "    global word_count_dict\n",
    "\n",
    "    words = txt.split(' ')\n",
    "    for word in words:\n",
    "        if word not in word_count_dict:\n",
    "            word_count_dict[word] = 0\n",
    "        word_count_dict[word] += 1\n",
    "\n",
    "for index, txt in corpus[\"text_clean\"].items():\n",
    "    count_words(txt)\n",
    "\n",
    "print(\"corpus size:\", len(word_count_dict))\n",
    "print(\"initial vocab size:\", len(char_count_dict))\n",
    "\n",
    "# Should be 64 (a-zA-Z0-9)\n",
    "print(\"initial vocab:\", char_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the \"merging\" strategy, essentially merge the most popular/frequent characters in the corpus into new characters, and subsequently replace those characters with our new characters.\n",
    "\n",
    "As an example: `m,a,t`, may become `m,at` if `at` is popular enough.\n",
    "\n",
    "To give an example, suppose we have example words: `cat`, `sat`, `hat`, `brat`, `broken`. We can see that common character pairs are `(a, t)`, `(b, r)`\n",
    "both of which appear more than once. When we apply a \"merge\", we will merge the most popular character pairs.\n",
    "\n",
    "The stopping function is typically when vocabulary size reaches a desired limit (ex. 1000), but I'd also like to try a stopping function based on the frequency of each pair. (ex. suppose frequency of `(x, y)` drops below 1% of total corpus and is our next best pair, we stop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_split_pairs(splits: dict[str, list[str]], word_counts: dict[str, int]) -> tuple[tuple[str, str], int]:\n",
    "    # identifies the top pair from splits, generates pair from splits\n",
    "    # this must be regenerated each time splits change\n",
    "    split_pair_count = {}    \n",
    "    for word, split in splits.items():\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "\n",
    "            if pair not in split_pair_count:\n",
    "                split_pair_count[pair] = 0\n",
    "            split_pair_count[pair] += word_counts[word]\n",
    "    \n",
    "    max_pair = None\n",
    "    max_count = 0\n",
    "    for pair, count in split_pair_count.items():\n",
    "        if count > max_count:\n",
    "            max_pair = pair\n",
    "            max_count = count\n",
    "    return (max_pair, max_count)\n",
    "\n",
    "def merge_splits(splits: dict[str, list[str]], pair: tuple[str, str]) -> None:\n",
    "    # TODO: is there a more efficient query structure? perhaps an inverse map\n",
    "    # from pairs -> words?\n",
    "    merged = pair[0] + pair[1]\n",
    "    for word, split in splits.items():\n",
    "        # there is probably a more efficient way to do this comparison\n",
    "        for i in range(len(split) - 1):\n",
    "            # because we're popping elements, i >= len(split) - 1, therefore\n",
    "            # we need to do an additional check to exit the loop\n",
    "            if i >= len(split) - 1:\n",
    "                break\n",
    "\n",
    "            if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                # if we find a match, we need to merge this entry\n",
    "                split[i] = merged\n",
    "                split.pop(i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the general strategy is:\n",
    "- From splits, generate a pair -> count table\n",
    "- Find the top ranked pair (max_pair)\n",
    "- Go back and update splits to replace sub-pair in split, to merged pair (ex. `(t, h) becomes th`)\n",
    "- Loop back to top, and find next max_pair, do this until criteria is met.\n",
    "\n",
    "Let's first try the target vocabulary criteria. We will run until we hit 100 vocabulary words (so roughly 40 extra new merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 max_pair, max_count ('t', 'h') 9702736\n",
      "2 max_pair, max_count ('i', 'n') 7911135\n",
      "3 max_pair, max_count ('e', 'r') 6860409\n",
      "4 max_pair, max_count ('a', 'n') 6479598\n",
      "5 max_pair, max_count ('th', 'e') 6222698\n",
      "6 max_pair, max_count ('o', 'n') 5550016\n",
      "7 max_pair, max_count ('e', 'd') 4747943\n",
      "8 max_pair, max_count ('r', 'e') 4108949\n",
      "9 max_pair, max_count ('a', 't') 4096933\n",
      "10 max_pair, max_count ('e', 'n') 3748855\n",
      "11 max_pair, max_count ('o', 'r') 3699380\n",
      "12 max_pair, max_count ('a', 'l') 3399189\n",
      "13 max_pair, max_count ('s', 't') 3390231\n",
      "14 max_pair, max_count ('a', 'r') 3330730\n",
      "15 max_pair, max_count ('an', 'd') 3044037\n",
      "16 max_pair, max_count ('a', 's') 3000753\n",
      "17 max_pair, max_count ('o', 'f') 2985590\n",
      "18 max_pair, max_count ('in', 'g') 2665518\n",
      "19 max_pair, max_count ('e', 's') 2607832\n",
      "20 max_pair, max_count ('t', 'o') 2587730\n",
      "21 max_pair, max_count ('i', 's') 2566371\n",
      "22 max_pair, max_count ('i', 't') 2385249\n",
      "23 max_pair, max_count ('o', 'u') 2225909\n",
      "24 max_pair, max_count ('i', 'c') 2149612\n",
      "25 max_pair, max_count ('i', 'on') 1882064\n",
      "26 max_pair, max_count ('l', 'e') 1808621\n",
      "27 max_pair, max_count ('h', 'e') 1784527\n",
      "28 max_pair, max_count ('r', 'o') 1760147\n",
      "29 max_pair, max_count ('i', 'l') 1411719\n",
      "30 max_pair, max_count ('a', 'c') 1371520\n",
      "31 max_pair, max_count ('en', 't') 1332133\n",
      "32 max_pair, max_count ('a', 'd') 1278000\n",
      "33 max_pair, max_count ('s', 'e') 1268553\n",
      "34 max_pair, max_count ('a', 'm') 1231903\n",
      "35 max_pair, max_count ('o', 'm') 1150624\n",
      "36 max_pair, max_count ('b', 'e') 1120611\n",
      "37 max_pair, max_count ('l', 'y') 1120494\n",
      "38 max_pair, max_count ('w', 'as') 1082133\n"
     ]
    }
   ],
   "source": [
    "# splits are responsible for keeping track of how each word is split\n",
    "# as an example: cat can be [\"c\", \"a\", \"t\"] or [\"c\", \"at\"], splits should represet\n",
    "# the current state of our vocabulary. So if we add the \"at\" token to our vocab, the splits\n",
    "# also need to get updated to reflect that, our character pair in turn is updated each time\n",
    "# split is updated, so that after sorting we have the new largest pair\n",
    "\n",
    "# intuitively one can see that (\"c\", \"a\"), will likely be more general than (\"c\", \"at\"), so BPE\n",
    "# will likely \"learn\" shorter pairs first (but not always, depending on corpus).\n",
    "# see: https://huggingface.co/learn/nlp-course/en/chapter6/5 for more info\n",
    "\n",
    "splits = {word: [c for c in word] for word in word_count_dict.keys()}\n",
    "\n",
    "# Begin runs here\n",
    "TARGET_VOCAB_SIZE = 100\n",
    "\n",
    "vocab_set = char_set.copy()\n",
    "it = 0\n",
    "while len(vocab_set) < TARGET_VOCAB_SIZE:\n",
    "    it += 1\n",
    "    (max_pair, max_count) = calc_split_pairs(splits=splits, word_counts=word_count_dict)\n",
    "    print(it, \"max_pair, max_count\", max_pair, max_count)\n",
    "    # then merge relevant splits so we can calculate again\n",
    "    vocab_set.add(max_pair[0] + max_pair[1])\n",
    "    merge_splits(splits=splits, pair=max_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the resulting vocabulary having ~40 (38) new elements. All that is left is to assign a unique index to each element. Imagine that the vocabulary is small enough such that each element in the vocab can be encoded in 1 byte. This vocabulary set would essentially \"compress\", the words it represented (ex. instead of `t, h, e`, being 3 bytes, it could be one `the`, or two `t, he`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting vocab set: {'t', 'G', '5', 'N', 'r', 'the', 'q', 'v', 'X', 'V', 'ic', 'an', 're', 's', 'se', 'in', 'le', 'en', 'x', 'was', 'am', 'B', 'R', 'to', 'L', 'A', '9', 'w', 'ent', '4', 'j', 'ed', 'i', 'c', 'o', 'z', '7', 'ou', 'g', '1', 'u', 'f', 'O', 'U', 'er', 'on', 'm', 'p', 'ro', 'J', 'om', 'T', '6', 'E', 'al', 'as', 'ar', 'C', '2', 'K', 'Z', 'd', 'it', 'n', 'b', 'be', 'D', 'P', 'ly', '8', 'of', 'ion', 'M', 'W', 'ac', 'ing', '0', 'or', 'es', 'e', '3', 'S', 'he', 'Y', 'and', 'at', 'il', 'l', 'is', 'ad', 'H', 'I', 'k', 'F', 'st', 'y', 'a', 'th', 'h', 'Q'}\n"
     ]
    }
   ],
   "source": [
    "print('resulting vocab set:', vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

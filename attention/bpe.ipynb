{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding\n",
    "- Start with a small vocabulary (for our ease of use let's just assume only ASCII characters)\n",
    "- \"Learn\" new vocabulary by scanning through corpus, use frequency of sub-sequence appearence to learn the most important new vocubulary\n",
    "\n",
    "For simplicity, let's limit our vocabulary to only uppercase and lowercase ASCII, plus numbers. We will first purge the corpus of all other symbols while preserving spaces, note that this is not a realistic scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 575352\n",
      "initial vocab size: 62\n",
      "initial vocab: {'x', 't', 'p', 'G', 'W', 'B', 'R', 'J', 'T', '0', 'L', 'e', '6', 'A', 'E', '3', '5', 'N', 'r', '9', 'S', 'w', 'q', 'v', '4', 'C', 'Y', '2', 'X', 'K', 'Z', 'V', 'd', 'j', 'l', 'i', 'c', 'o', 's', 'n', 'H', 'z', 'b', 'I', 'k', '7', 'D', 'F', 'g', 'P', '1', 'u', 'f', '8', 'O', 'U', 'y', 'a', 'M', 'h', 'm', 'Q'}\n"
     ]
    }
   ],
   "source": [
    "# Load corpus, read all lines into memory, clean, then extract all characters, whitespace symbols\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "WIKITEXT_PATH = \"../data/Salesforce/wikitext/wikitext-103-raw-v1\"\n",
    "TRAIN_SET = [\"train-00000-of-00002.parquet\", \"train-00001-of-00002.parquet\"]\n",
    "TRAIN_SET_PATHS = [os.path.join(WIKITEXT_PATH, p) for p in TRAIN_SET]\n",
    "corpus = pq.ParquetDataset(TRAIN_SET_PATHS).read().to_pandas()\n",
    "\n",
    "# Clean the corpus of whitespace symbols (assume they're not useful to us)\n",
    "clear_ws_pattern = re.compile(r\"[^a-zA-Z0-9]\")\n",
    "\n",
    "def clean_corpus(txt: str) -> str:\n",
    "    spaced_txt = clear_ws_pattern.sub(' ', txt)\n",
    "    return ' '.join(spaced_txt.strip().split())\n",
    "\n",
    "corpus[\"text_clean\"] = corpus[\"text\"].apply(clean_corpus)\n",
    "\n",
    "# Now start a dictionary of character counts (importance)\n",
    "char_count_dict = {\n",
    "    \"a\": 0  # as an example\n",
    "}\n",
    "\n",
    "def count_chars(txt: str) -> None:\n",
    "    global char_count_dict\n",
    "    # kind of like bag of words (bag of chars?)\n",
    "    for c in list(txt):\n",
    "        if c not in char_count_dict:\n",
    "            char_count_dict[c] = 0\n",
    "        char_count_dict[c] += 1\n",
    "\n",
    "# Okay now the \"cleaning\" is done, we can start applying character and word counts\n",
    "# Count character sum in corpus (not _really_ needded, just for viz)\n",
    "for index, txt in corpus[\"text_clean\"].items():\n",
    "    count_chars(txt)\n",
    "\n",
    "# Removal all empty rows\n",
    "corpus = corpus[corpus[\"text_clean\"].str.len() > 0]\n",
    "\n",
    "# Count characters, then sort into descending order, delete the space parameter (considered whitespace)\n",
    "# Note: the sorting is purely for visualization, it is not needed\n",
    "del char_count_dict[' ']\n",
    "char_count_dict = dict(sorted(char_count_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# This forms our initial character set\n",
    "# Note: why are we going through all this trouble to get a corpus? We originally extended to chars\n",
    "# beyond just alphanum\n",
    "char_set = set(char_count_dict.keys())\n",
    "\n",
    "# Now we need to generate our initial word set, same as char_count_dict\n",
    "# (basically bag-of-words)\n",
    "word_count_dict = {}\n",
    "\n",
    "def count_words(txt: str) -> None:\n",
    "    global word_count_dict\n",
    "\n",
    "    words = txt.split(' ')\n",
    "    for word in words:\n",
    "        if word not in word_count_dict:\n",
    "            word_count_dict[word] = 0\n",
    "        word_count_dict[word] += 1\n",
    "\n",
    "for index, txt in corpus[\"text_clean\"].items():\n",
    "    count_words(txt)\n",
    "\n",
    "print(\"corpus size:\", len(word_count_dict))\n",
    "print(\"initial vocab size:\", len(char_count_dict))\n",
    "\n",
    "# Should be 64 (a-zA-Z0-9)\n",
    "print(\"initial vocab:\", char_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the \"merging\" strategy, essentially merge the most popular/frequent characters in the corpus into new characters, and subsequently replace those characters with our new characters.\n",
    "\n",
    "As an example: `m,a,t`, may become `m,at` if `at` is popular enough.\n",
    "\n",
    "To give an example, suppose we have example words: `cat`, `sat`, `hat`, `brat`, `broken`. We can see that common character pairs are `(a, t)`, `(b, r)`\n",
    "both of which appear more than once. When we apply a \"merge\", we will merge the most popular character pairs.\n",
    "\n",
    "The stopping function is typically when vocabulary size reaches a desired limit (ex. 1000), but I'd also like to try a stopping function based on the frequency of each pair. (ex. suppose frequency of `(x, y)` drops below 1% of total corpus and is our next best pair, we stop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_split_pairs(splits: dict[str, list[str]], word_counts: dict[str, int]) -> tuple[tuple[str, str], int]:\n",
    "    # identifies the top pair from splits, generates pair from splits\n",
    "    # this must be regenerated each time splits change\n",
    "    split_pair_count = {}    \n",
    "    for word, split in splits.items():\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "\n",
    "            if pair not in split_pair_count:\n",
    "                split_pair_count[pair] = 0\n",
    "            split_pair_count[pair] += word_counts[word]\n",
    "    \n",
    "    max_pair = None\n",
    "    max_count = 0\n",
    "    for pair, count in split_pair_count.items():\n",
    "        if count > max_count:\n",
    "            max_pair = pair\n",
    "            max_count = count\n",
    "    return (max_pair, max_count)\n",
    "\n",
    "def merge_splits(splits: dict[str, list[str]], pair: tuple[str, str]) -> None:\n",
    "    # TODO: is there a more efficient query structure? perhaps an inverse map\n",
    "    # from pairs -> words?\n",
    "    merged = pair[0] + pair[1]\n",
    "    for word, split in splits.items():\n",
    "        # there is probably a more efficient way to do this comparison\n",
    "        for i in range(len(split) - 1):\n",
    "            # because we're popping elements, i >= len(split) - 1, therefore\n",
    "            # we need to do an additional check to exit the loop\n",
    "            if i >= len(split) - 1:\n",
    "                break\n",
    "\n",
    "            if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                # if we find a match, we need to merge this entry\n",
    "                split[i] = merged\n",
    "                split.pop(i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the general strategy is:\n",
    "- From splits, generate a pair -> count table\n",
    "- Find the top ranked pair (max_pair)\n",
    "- Go back and update splits to replace sub-pair in split, to merged pair (ex. `(t, h) becomes th`)\n",
    "- Loop back to top, and find next max_pair, do this until criteria is met.\n",
    "\n",
    "Let's first try the target vocabulary criteria. We will run until we hit 100 vocabulary words (so roughly 40 extra new merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 max_pair, max_count ('t', 'h') 9702736\n",
      "2 max_pair, max_count ('i', 'n') 7911135\n",
      "3 max_pair, max_count ('e', 'r') 6860409\n",
      "4 max_pair, max_count ('a', 'n') 6479598\n",
      "5 max_pair, max_count ('th', 'e') 6222698\n",
      "6 max_pair, max_count ('o', 'n') 5550016\n",
      "7 max_pair, max_count ('e', 'd') 4747943\n",
      "8 max_pair, max_count ('r', 'e') 4108949\n",
      "9 max_pair, max_count ('a', 't') 4096933\n",
      "10 max_pair, max_count ('e', 'n') 3748855\n",
      "11 max_pair, max_count ('o', 'r') 3699380\n",
      "12 max_pair, max_count ('a', 'l') 3399189\n",
      "13 max_pair, max_count ('s', 't') 3390231\n",
      "14 max_pair, max_count ('a', 'r') 3330730\n",
      "15 max_pair, max_count ('an', 'd') 3044037\n",
      "16 max_pair, max_count ('a', 's') 3000753\n",
      "17 max_pair, max_count ('o', 'f') 2985590\n",
      "18 max_pair, max_count ('in', 'g') 2665518\n",
      "19 max_pair, max_count ('e', 's') 2607832\n",
      "20 max_pair, max_count ('t', 'o') 2587730\n",
      "21 max_pair, max_count ('i', 's') 2566371\n",
      "22 max_pair, max_count ('i', 't') 2385249\n",
      "23 max_pair, max_count ('o', 'u') 2225909\n",
      "24 max_pair, max_count ('i', 'c') 2149612\n",
      "25 max_pair, max_count ('i', 'on') 1882064\n",
      "26 max_pair, max_count ('l', 'e') 1808621\n",
      "27 max_pair, max_count ('h', 'e') 1784527\n",
      "28 max_pair, max_count ('r', 'o') 1760147\n",
      "29 max_pair, max_count ('i', 'l') 1411719\n",
      "30 max_pair, max_count ('a', 'c') 1371520\n",
      "31 max_pair, max_count ('en', 't') 1332133\n",
      "32 max_pair, max_count ('a', 'd') 1278000\n",
      "33 max_pair, max_count ('s', 'e') 1268553\n",
      "34 max_pair, max_count ('a', 'm') 1231903\n",
      "35 max_pair, max_count ('o', 'm') 1150624\n",
      "36 max_pair, max_count ('b', 'e') 1120611\n",
      "37 max_pair, max_count ('l', 'y') 1120494\n",
      "38 max_pair, max_count ('w', 'as') 1082133\n"
     ]
    }
   ],
   "source": [
    "# splits are responsible for keeping track of how each word is split\n",
    "# as an example: cat can be [\"c\", \"a\", \"t\"] or [\"c\", \"at\"], splits should represet\n",
    "# the current state of our vocabulary. So if we add the \"at\" token to our vocab, the splits\n",
    "# also need to get updated to reflect that, our character pair in turn is updated each time\n",
    "# split is updated, so that after sorting we have the new largest pair\n",
    "\n",
    "# intuitively one can see that (\"c\", \"a\"), will likely be more general than (\"c\", \"at\"), so BPE\n",
    "# will likely \"learn\" shorter pairs first (but not always, depending on corpus).\n",
    "# see: https://huggingface.co/learn/nlp-course/en/chapter6/5 for more info\n",
    "\n",
    "splits = {word: [c for c in word] for word in word_count_dict.keys()}\n",
    "\n",
    "# Begin runs here\n",
    "TARGET_VOCAB_SIZE = 100\n",
    "\n",
    "vocab_set = char_set.copy()\n",
    "it = 0\n",
    "while len(vocab_set) < TARGET_VOCAB_SIZE:\n",
    "    it += 1\n",
    "    (max_pair, max_count) = calc_split_pairs(splits=splits, word_counts=word_count_dict)\n",
    "    print(it, \"max_pair, max_count\", max_pair, max_count)\n",
    "    # then merge relevant splits so we can calculate again\n",
    "    vocab_set.add(max_pair[0] + max_pair[1])\n",
    "    merge_splits(splits=splits, pair=max_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the resulting vocabulary having ~40 (38) new elements. All that is left is to assign a unique index to each element. Imagine that the vocabulary is small enough such that each element in the vocab can be encoded in 1 byte. This vocabulary set would essentially \"compress\", the words it represented (ex. instead of `t, h, e`, being 3 bytes, it could be one `the`, or two `t, he`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting vocab set: {'t', 'G', '5', 'N', 'r', 'the', 'q', 'v', 'X', 'V', 'ic', 'an', 're', 's', 'se', 'in', 'le', 'en', 'x', 'was', 'am', 'B', 'R', 'to', 'L', 'A', '9', 'w', 'ent', '4', 'j', 'ed', 'i', 'c', 'o', 'z', '7', 'ou', 'g', '1', 'u', 'f', 'O', 'U', 'er', 'on', 'm', 'p', 'ro', 'J', 'om', 'T', '6', 'E', 'al', 'as', 'ar', 'C', '2', 'K', 'Z', 'd', 'it', 'n', 'b', 'be', 'D', 'P', 'ly', '8', 'of', 'ion', 'M', 'W', 'ac', 'ing', '0', 'or', 'es', 'e', '3', 'S', 'he', 'Y', 'and', 'at', 'il', 'l', 'is', 'ad', 'H', 'I', 'k', 'F', 'st', 'y', 'a', 'th', 'h', 'Q'}\n"
     ]
    }
   ],
   "source": [
    "print('resulting vocab set:', vocab_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try an alternative method, where we keep creating vocabulary sets until the \"frequency\" of the word falls below some threshold. To do this we need to modify `calc_split_pairs` to return an additional metric.\n",
    "\n",
    "We define expectation as `1 / (max_count / total_count)`, so if the expectation is 200, then we should expect to see the pairing in roughly 1 out of every 200 pairs. An expectation of 1 means this pairing is present in every pair. So we want to check that our expectation is below some threshold `N`.\n",
    "\n",
    "We have not fundamentally changed anything about the algo, since expectation in this sense is still a surrogate for popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 max_pair, expectation ('t', 'h') 33.9099562226572\n",
      "2 max_pair, expectation ('i', 'n') 40.36293363720882\n",
      "3 max_pair, expectation ('e', 'r') 45.391678834308564\n",
      "4 max_pair, expectation ('a', 'n') 47.00061222933892\n",
      "5 max_pair, expectation ('th', 'e') 47.899717293045555\n",
      "6 max_pair, expectation ('o', 'n') 52.584132550248505\n",
      "7 max_pair, expectation ('e', 'd') 60.29827253612775\n",
      "8 max_pair, expectation ('r', 'e') 68.5199105659379\n",
      "9 max_pair, expectation ('a', 't') 67.71794144546665\n",
      "10 max_pair, expectation ('e', 'n') 72.91264559445484\n",
      "11 max_pair, expectation ('o', 'r') 72.87439543923577\n",
      "12 max_pair, expectation ('a', 'l') 78.2218055542072\n",
      "13 max_pair, expectation ('s', 't') 77.4258485631215\n",
      "14 max_pair, expectation ('a', 'r') 77.79113917969934\n",
      "15 max_pair, expectation ('an', 'd') 84.02346981984779\n",
      "16 max_pair, expectation ('a', 's') 84.2210318543379\n",
      "17 max_pair, expectation ('o', 'f') 83.64368885211968\n",
      "18 max_pair, expectation ('in', 'g') 92.56743754872412\n",
      "19 max_pair, expectation ('e', 's') 93.59293581795146\n",
      "20 max_pair, expectation ('t', 'o') 93.312216112191\n",
      "21 max_pair, expectation ('i', 's') 93.08049810413226\n",
      "22 max_pair, expectation ('i', 't') 99.0725580432064\n",
      "23 max_pair, expectation ('o', 'u') 105.09300739607953\n",
      "24 max_pair, expectation ('i', 'c') 107.78762027751985\n",
      "25 max_pair, expectation ('i', 'on') 121.96819555551778\n",
      "26 max_pair, expectation ('l', 'e') 125.8803729471238\n",
      "27 max_pair, expectation ('h', 'e') 126.56645990786353\n",
      "28 max_pair, expectation ('r', 'o') 127.30569549020622\n",
      "29 max_pair, expectation ('i', 'l') 157.479350352301\n",
      "30 max_pair, expectation ('a', 'c') 161.0657314512366\n",
      "31 max_pair, expectation ('en', 't') 164.7983737359558\n",
      "32 max_pair, expectation ('a', 'd') 170.73647809076684\n",
      "33 max_pair, expectation ('s', 'e') 171.0005171246294\n",
      "34 max_pair, expectation ('a', 'm') 175.05815474107948\n",
      "35 max_pair, expectation ('o', 'm') 186.3534595141419\n",
      "36 max_pair, expectation ('b', 'e') 190.31772756112514\n",
      "37 max_pair, expectation ('l', 'y') 189.33749578310994\n",
      "38 max_pair, expectation ('w', 'as') 195.01395299838376\n",
      "39 max_pair, expectation ('f', 'or') 195.33837211898816\n",
      "40 max_pair, expectation ('c', 'h') 195.26273057694252\n",
      "41 max_pair, expectation ('v', 'e') 207.67294002896165\n",
      "42 max_pair, expectation ('d', 'e') 206.71219049960865\n",
      "43 max_pair, expectation ('u', 'r') 207.3883904683496\n",
      "44 max_pair, expectation ('o', 'l') 216.60969403403382\n",
      "45 max_pair, expectation ('i', 'r') 220.281708007555\n",
      "46 max_pair, expectation ('T', 'he') 221.23860284097725\n",
      "47 max_pair, expectation ('i', 'm') 224.1384043580043\n",
      "48 max_pair, expectation ('i', 'g') 233.04743883588293\n",
      "49 max_pair, expectation ('v', 'er') 239.97460988522442\n",
      "50 max_pair, expectation ('t', 'er') 240.49597754031203\n",
      "51 max_pair, expectation ('w', 'h') 242.18323866061476\n",
      "52 max_pair, expectation ('o', 'w') 242.04721016386182\n",
      "53 max_pair, expectation ('u', 'n') 249.1077060451374\n",
      "54 max_pair, expectation ('i', 'th') 250.29135927854915\n",
      "55 max_pair, expectation ('c', 'e') 252.66504502197756\n",
      "56 max_pair, expectation ('a', 'y') 256.3326664102591\n",
      "57 max_pair, expectation ('i', 'd') 271.59530114270024\n",
      "58 max_pair, expectation ('c', 'on') 272.3367348953163\n",
      "59 max_pair, expectation ('at', 'ion') 272.7033587007355\n",
      "60 max_pair, expectation ('c', 't') 271.9963127952672\n",
      "61 max_pair, expectation ('w', 'ith') 273.19428517489297\n",
      "62 max_pair, expectation ('a', 'g') 278.2292257829666\n",
      "63 max_pair, expectation ('th', 'at') 278.7824254434582\n",
      "64 max_pair, expectation ('t', 'e') 283.1407809107379\n",
      "65 max_pair, expectation ('b', 'y') 289.32058980804806\n",
      "66 max_pair, expectation ('u', 's') 290.34698860805327\n",
      "67 max_pair, expectation ('o', 't') 301.93025820546313\n",
      "68 max_pair, expectation ('e', 'l') 304.26814678401814\n",
      "69 max_pair, expectation ('er', 'e') 308.6631594864186\n",
      "70 max_pair, expectation ('r', 'i') 312.80684454070416\n",
      "71 max_pair, expectation ('u', 'l') 318.46322891233643\n",
      "72 max_pair, expectation ('a', 'p') 320.96728818704224\n",
      "73 max_pair, expectation ('e', 't') 331.0757673944241\n",
      "74 max_pair, expectation ('u', 't') 330.9791139114491\n",
      "75 max_pair, expectation ('1', '9') 332.19243538535545\n",
      "76 max_pair, expectation ('re', 's') 331.7419211766016\n",
      "77 max_pair, expectation ('a', 'in') 335.09852060063616\n",
      "78 max_pair, expectation ('er', 's') 337.2630906911885\n",
      "79 max_pair, expectation ('o', 'p') 337.01174564449497\n",
      "80 max_pair, expectation ('p', 'l') 342.8665623031252\n",
      "81 max_pair, expectation ('c', 'om') 342.60499999043395\n",
      "82 max_pair, expectation ('h', 'is') 348.0101644833655\n",
      "83 max_pair, expectation ('ro', 'm') 347.66475361017\n",
      "84 max_pair, expectation ('u', 'm') 349.8269578651575\n",
      "85 max_pair, expectation ('al', 'l') 357.24008556754086\n",
      "86 max_pair, expectation ('0', '0') 357.0911691285649\n",
      "87 max_pair, expectation ('a', 'b') 356.4819122257688\n",
      "88 max_pair, expectation ('ou', 'n') 359.03327227828026\n",
      "89 max_pair, expectation ('i', 'st') 374.54963911848677\n",
      "90 max_pair, expectation ('m', 'o') 374.19142336274615\n",
      "91 max_pair, expectation ('s', 'u') 374.9270965603626\n",
      "92 max_pair, expectation ('n', 'e') 387.5181979988669\n",
      "93 max_pair, expectation ('s', 'h') 388.73958688512823\n",
      "94 max_pair, expectation ('p', 'ro') 389.72587889391946\n",
      "95 max_pair, expectation ('p', 'e') 398.7192110935346\n",
      "96 max_pair, expectation ('f', 'rom') 398.12525841812226\n",
      "97 max_pair, expectation ('I', 'n') 402.43855981907416\n",
      "98 max_pair, expectation ('c', 'l') 406.8175937196873\n",
      "99 max_pair, expectation ('at', 'e') 413.5174158882127\n",
      "100 max_pair, expectation ('a', 're') 414.8864207276812\n",
      "101 max_pair, expectation ('l', 'i') 418.5953081491338\n",
      "102 max_pair, expectation ('t', 'r') 419.0140145783264\n",
      "103 max_pair, expectation ('ar', 't') 419.567021310918\n",
      "104 max_pair, expectation ('th', 'er') 428.29602118917376\n",
      "105 max_pair, expectation ('o', 's') 431.2019430114884\n",
      "106 max_pair, expectation ('ig', 'h') 435.73490385813363\n",
      "107 max_pair, expectation ('e', 'm') 441.425072148646\n",
      "108 max_pair, expectation ('d', 'i') 441.59591088185323\n",
      "109 max_pair, expectation ('e', 'x') 450.2233539547481\n",
      "110 max_pair, expectation ('b', 'er') 458.0043251190027\n",
      "111 max_pair, expectation ('e', 'st') 460.6772939325231\n",
      "112 max_pair, expectation ('ic', 'h') 463.00400568007115\n",
      "113 max_pair, expectation ('a', 'k') 464.38732980056534\n",
      "114 max_pair, expectation ('w', 'ere') 464.9445215018296\n",
      "115 max_pair, expectation ('l', 'd') 464.06619179349224\n",
      "116 max_pair, expectation ('o', 'c') 465.28884443525146\n",
      "117 max_pair, expectation ('il', 'l') 465.39653267516263\n",
      "118 max_pair, expectation ('ar', 'd') 464.67500483124735\n",
      "119 max_pair, expectation ('a', 'st') 475.9249224809593\n",
      "120 max_pair, expectation ('i', 'es') 485.0686339937435\n",
      "121 max_pair, expectation ('an', 't') 485.4338942235873\n",
      "122 max_pair, expectation ('p', 'er') 489.3536598247727\n",
      "123 max_pair, expectation ('on', 'g') 488.59254360934716\n",
      "124 max_pair, expectation ('q', 'u') 489.4502670328139\n",
      "125 max_pair, expectation ('g', 'h') 498.3316500711237\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m expectation \u001b[38;5;241m<\u001b[39m N:\n\u001b[1;32m     38\u001b[0m     it \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 40\u001b[0m     (max_pair, max_count, expectation) \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_split_pairs_freq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplits_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword_count_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m expectation \u001b[38;5;241m>\u001b[39m N:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m, in \u001b[0;36mcalc_split_pairs_freq\u001b[0;34m(splits, word_counts)\u001b[0m\n\u001b[1;32m     22\u001b[0m         word_count \u001b[38;5;241m=\u001b[39m word_counts[word]\n\u001b[1;32m     23\u001b[0m         total_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m word_count\n\u001b[0;32m---> 24\u001b[0m         split_pair_count[pair] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m word_count\n\u001b[1;32m     26\u001b[0m max_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     27\u001b[0m max_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Add freq suffix to not override, distinguish from previous run\n",
    "splits_freq = {word: [c for c in word] for word in word_count_dict.keys()}\n",
    "\n",
    "# Begin runs here\n",
    "N = 100  # We want to capture all pairings that appear atleast once every 100 pairings (expected)\n",
    "\n",
    "vocab_set_freq = char_set.copy()\n",
    "\n",
    "def calc_split_pairs_freq(splits: dict[str, list[str]], word_counts: dict[str, int]) -> tuple[tuple[str, str], int]:\n",
    "    # identifies the top pair from splits, generates pair from splits\n",
    "    # this must be regenerated each time splits change\n",
    "    split_pair_count = {}\n",
    "    total_count = 0\n",
    "    for word, split in splits.items():\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "\n",
    "            if pair not in split_pair_count:\n",
    "                split_pair_count[pair] = 0\n",
    "            \n",
    "            # add total counter\n",
    "            word_count = word_counts[word]\n",
    "            total_count += word_count\n",
    "            split_pair_count[pair] += word_count\n",
    "    \n",
    "    max_pair = None\n",
    "    max_count = 0\n",
    "    for pair, count in split_pair_count.items():\n",
    "        if count > max_count:\n",
    "            max_pair = pair\n",
    "            max_count = count\n",
    "    return (max_pair, max_count, 1 / (max_count / total_count))\n",
    "\n",
    "\n",
    "expectation = 0\n",
    "it = 0\n",
    "while expectation < N:\n",
    "    it += 1\n",
    "\n",
    "    (max_pair, max_count, expectation) = calc_split_pairs_freq(splits=splits_freq, word_counts=word_count_dict)\n",
    "    \n",
    "    if expectation > N:\n",
    "        break\n",
    "    \n",
    "    vocab_set_freq.add(max_pair[0] + max_pair[1])\n",
    "    print(it, \"max_pair, expectation\", max_pair, expectation)\n",
    "    merge_splits(splits=splits_freq, pair=max_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
